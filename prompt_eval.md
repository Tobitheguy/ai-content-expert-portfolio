# Prompt Evaluation: Teaching Inflation

## üß† Goal
Evaluate two prompts and their impact on the quality of LLM-generated explanations.

---

## üìù Prompts

**Prompt A:**
> Explain inflation.

**Prompt B:**
> You are a teacher explaining inflation to a 10-year-old using simple words and examples.

---

## üìä Output Comparison

| Prompt   | Output Clarity | Age Fit | Structure | Best? |
|----------|----------------|---------|-----------|-------|
| Prompt A | Medium         | No      | Flat      | ‚ùå    |
| Prompt B | High           | Yes     | Clear     | ‚úÖ    |

---

## üß† Analysis

**Prompt A** is vague. The AI output was short, somewhat technical, and lacked structure or examples. It might confuse someone without an economics background.

**Prompt B** gave much better results. The LLM explained inflation using a relatable example (e.g. "buying candy with pocket money") and structured the explanation with steps ‚Äî showing how good prompting leads to better learning.

---

## ‚úÖ Takeaway

This comparison shows how prompt engineering improves LLM outputs. Clear, contextual instructions help the model deliver more accurate, structured, and audience-specific answers.

